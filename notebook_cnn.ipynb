{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "import math\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data as mnist_data\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we create a CNN network for the MNIST dataset with the following architecture:\n",
    "* **Conv. layer 1:** computes 32 feature maps using a 5x5 filter with ReLU activation.\n",
    "* **Pooling layer 1:** max pooling layer with a 2x2 filter and stride of 2.\n",
    "* **Conv. layer 2:** computes 64 feature maps using a 5x5 filter.\n",
    "* **Pooling layer 2:** max pooling layer with a 2x2 filter and stride of 2.\n",
    "* **Dense layer:** densely connected layer with 1024 neurons.\n",
    "* **Logits layer**\n",
    "\n",
    "First we make the network manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-3-ae6516f51ef4>:10: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use urllib or similar directly.\n",
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From <ipython-input-3-ae6516f51ef4>:81: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "epoch 0: accurecy = 0.12999999523162842, loss = 819.0568237304688\n",
      "epoch 100: accurecy = 0.9399999976158142, loss = 21.723081588745117\n",
      "epoch 200: accurecy = 0.9200000166893005, loss = 23.565454483032227\n",
      "epoch 300: accurecy = 0.9599999785423279, loss = 20.058218002319336\n",
      "epoch 400: accurecy = 0.9100000262260437, loss = 26.220443725585938\n",
      "epoch 500: accurecy = 0.9300000071525574, loss = 18.892047882080078\n",
      "epoch 600: accurecy = 0.9800000190734863, loss = 15.38364315032959\n",
      "epoch 700: accurecy = 0.9800000190734863, loss = 5.333933353424072\n",
      "epoch 800: accurecy = 0.9700000286102295, loss = 11.36990737915039\n",
      "epoch 900: accurecy = 0.9800000190734863, loss = 6.2215752601623535\n",
      "test data: accurecy = 0.9787999987602234, loss = 6.626026153564453\n"
     ]
    }
   ],
   "source": [
    "# manual building layers\n",
    "reset_graph()\n",
    "\n",
    "n_epochs = 1000\n",
    "batch_size = 100\n",
    "\n",
    "########################################\n",
    "# load the mnist data\n",
    "########################################\n",
    "mnist = mnist_data.read_data_sets(\"data\", one_hot=True, reshape=False, validation_size=0)\n",
    "\n",
    "########################################\n",
    "# define placeholders\n",
    "########################################\n",
    "X = tf.placeholder(tf.float32, [None, 28, 28, 1])\n",
    "y_true = tf.placeholder(tf.float32, [None, 10])\n",
    "pkeep = tf.placeholder(tf.float32)\n",
    "\n",
    "########################################\n",
    "# build the model\n",
    "########################################\n",
    "# Convolutional Layer #1\n",
    "# Computes 32 feature maps using a 5x5 filter with ReLU activation.\n",
    "# Padding is added to preserve width and height.\n",
    "# Input Tensor Shape: [batch_size, 28, 28, 1]\n",
    "# Output Tensor Shape: [batch_size, 28, 28, 32]\n",
    "W1 = tf.Variable(tf.truncated_normal([5, 5, 1, 32], stddev=0.1))  # 5x5 filter, 1 input channel, 32 output channels\n",
    "B1 = tf.Variable(tf.constant(0.1, tf.float32, [32]))\n",
    "stride = 1  # output is 28x28\n",
    "conv1 = tf.nn.relu(tf.nn.conv2d(X, W1, strides=[1, stride, stride, 1], padding=\"SAME\") + B1)\n",
    "\n",
    "# Pooling Layer #1\n",
    "# First max pooling layer with a 2x2 filter and stride of 2\n",
    "# Input Tensor Shape: [batch_size, 28, 28, 32]\n",
    "# Output Tensor Shape: [batch_size, 14, 14, 32]\n",
    "pool1 = tf.nn.max_pool(conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"SAME\")\n",
    "\n",
    "# Convolutional Layer #2\n",
    "# Computes 64 features using a 5x5 filter.\n",
    "# Padding is added to preserve width and height.\n",
    "# Input Tensor Shape: [batch_size, 14, 14, 32]\n",
    "# Output Tensor Shape: [batch_size, 14, 14, 64]\n",
    "W2 = tf.Variable(tf.truncated_normal([5, 5, 32, 64], stddev=0.1))  # 5x5 filter, 32 input channel, 64 output channels\n",
    "B2 = tf.Variable(tf.constant(0.1, tf.float32, [64]))\n",
    "stride = 1  # output is 28x28\n",
    "conv2 = tf.nn.relu(tf.nn.conv2d(pool1, W2, strides=[1, stride, stride, 1], padding=\"SAME\") + B2)\n",
    "\n",
    "# Pooling Layer #2\n",
    "# Second max pooling layer with a 2x2 filter and stride of 2\n",
    "# Input Tensor Shape: [batch_size, 14, 14, 64]\n",
    "# Output Tensor Shape: [batch_size, 7, 7, 64]\n",
    "pool2 = tf.nn.max_pool(conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"SAME\")\n",
    "\n",
    "# Flatten tensor into a batch of vectors\n",
    "# Input Tensor Shape: [batch_size, 7, 7, 64]\n",
    "# Output Tensor Shape: [batch_size, 7 * 7 * 64]\n",
    "pool2_flat = tf.reshape(pool2, [-1, 7 * 7 * 64])\n",
    "\n",
    "# Dense Layer\n",
    "# Densely connected layer with 1024 neurons\n",
    "# Input Tensor Shape: [batch_size, 7 * 7 * 64]\n",
    "# Output Tensor Shape: [batch_size, 1024]\n",
    "W3 = tf.Variable(tf.truncated_normal([7 * 7 * 64, 1024], stddev=0.1))  # 7x7x64 feature, 1024 neurons\n",
    "B3 = tf.Variable(tf.constant(0.1, tf.float32, [1024]))\n",
    "dense = tf.nn.relu(tf.matmul(pool2_flat, W3) + B3)\n",
    "\n",
    "# Add dropout operation\n",
    "dropout = tf.nn.dropout(dense, pkeep)\n",
    "\n",
    "# Logits layer\n",
    "# Input Tensor Shape: [batch_size, 1024]\n",
    "# Output Tensor Shape: [batch_size, 10]\n",
    "W4 = tf.Variable(tf.truncated_normal([1024, 10], stddev=0.1))  # 1024 feature, 10 neurons\n",
    "B4 = tf.Variable(tf.constant(0.1, tf.float32, [10]))\n",
    "logits = tf.matmul(dropout, W4) + B4\n",
    "y_hat = tf.nn.softmax(logits)\n",
    "\n",
    "########################################\n",
    "# define the cost and accuracy functions\n",
    "########################################\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_true)\n",
    "cross_entropy = tf.reduce_mean(cross_entropy) * 100\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y_hat, 1), tf.argmax(y_true, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "########################################\n",
    "# define the optimizer\n",
    "########################################\n",
    "lr = 0.003\n",
    "optimizer = tf.train.AdamOptimizer(lr)\n",
    "train_step = optimizer.minimize(cross_entropy)\n",
    "\n",
    "########################################\n",
    "# execute the model\n",
    "########################################\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for i in range(n_epochs):\n",
    "        # load batch of images and correct answers\n",
    "        batch_X, batch_y = mnist.train.next_batch(batch_size)\n",
    "        \n",
    "        # learning rate decay\n",
    "        if i % 100 == 0:\n",
    "            a, c = sess.run([accuracy, cross_entropy], feed_dict={X: batch_X, y_true: batch_y, pkeep: 1.0})\n",
    "            print('epoch {}: accurecy = {}, loss = {}'.format(i, a, c))\n",
    "\n",
    "        # train\n",
    "        sess.run(train_step, feed_dict={X: batch_X, y_true: batch_y, pkeep: 0.75})\n",
    "        \n",
    "    a, c = sess.run([accuracy, cross_entropy], feed_dict={X: mnist.test.images, y_true: mnist.test.labels, pkeep: 1.0})\n",
    "    print('test data: accurecy = {}, loss = {}'.format(a, c))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we take advantage of the `tf.layers` to build the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_epochs = 1000\n",
    "batch_size = 100\n",
    "\n",
    "########################################\n",
    "# load the mnist data\n",
    "########################################\n",
    "mnist = mnist_data.read_data_sets(\"data\", one_hot=True, reshape=False, validation_size=0)\n",
    "\n",
    "#######################################\n",
    "# defineplaceholders\n",
    "########################################\n",
    "X = tf.placeholder(tf.float32, [None, 28, 28, 1])\n",
    "y_true = tf.placeholder(tf.float32, [None, 10])\n",
    "pkeep = tf.placeholder(tf.float32)\n",
    "\n",
    "########################################\n",
    "# build the model\n",
    "########################################\n",
    "# Convolutional Layer #1\n",
    "# Computes 32 feature maps using a 5x5 filter with ReLU activation.\n",
    "# Padding is added to preserve width and height.\n",
    "# Input Tensor Shape: [batch_size, 28, 28, 1]\n",
    "# Output Tensor Shape: [batch_size, 28, 28, 32]\n",
    "conv1 = tf.layers.conv2d(inputs=X, filters=32, kernel_size=[5, 5], padding=\"same\", activation=tf.nn.relu)\n",
    "\n",
    "# Pooling Layer #1\n",
    "# First max pooling layer with a 2x2 filter and stride of 2\n",
    "# Input Tensor Shape: [batch_size, 28, 28, 32]\n",
    "# Output Tensor Shape: [batch_size, 14, 14, 32]\n",
    "pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], strides=2)\n",
    "\n",
    "# Convolutional Layer #2\n",
    "# Computes 64 features using a 5x5 filter.\n",
    "# Padding is added to preserve width and height.\n",
    "# Input Tensor Shape: [batch_size, 14, 14, 32]\n",
    "# Output Tensor Shape: [batch_size, 14, 14, 64]\n",
    "conv2 = tf.layers.conv2d(inputs=pool1, filters=64, kernel_size=[5, 5], padding=\"same\", activation=tf.nn.relu)\n",
    "\n",
    "# Pooling Layer #2\n",
    "# Second max pooling layer with a 2x2 filter and stride of 2\n",
    "# Input Tensor Shape: [batch_size, 14, 14, 64]\n",
    "# Output Tensor Shape: [batch_size, 7, 7, 64]\n",
    "pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], strides=2)\n",
    "\n",
    "# Flatten tensor into a batch of vectors\n",
    "# Input Tensor Shape: [batch_size, 7, 7, 64]\n",
    "# Output Tensor Shape: [batch_size, 7 * 7 * 64]\n",
    "pool2_flat = tf.reshape(pool2, [-1, 7 * 7 * 64])\n",
    "\n",
    "# Dense Layer\n",
    "# Densely connected layer with 1024 neurons\n",
    "# Input Tensor Shape: [batch_size, 7 * 7 * 64]\n",
    "# Output Tensor Shape: [batch_size, 1024]\n",
    "dense = tf.layers.dense(inputs=pool2_flat, units=1024, activation=tf.nn.relu)\n",
    "\n",
    "# Add dropout operation\n",
    "dropout = tf.layers.dropout(inputs=dense, rate=pkeep)\n",
    "\n",
    "# Logits layer\n",
    "# Input Tensor Shape: [batch_size, 1024]\n",
    "# Output Tensor Shape: [batch_size, 10]\n",
    "logits = tf.layers.dense(inputs=dropout, units=10)\n",
    "y_hat = tf.nn.softmax(logits)\n",
    "\n",
    "########################################\n",
    "# define the cost and accuracy functions\n",
    "########################################\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_true)\n",
    "cross_entropy = tf.reduce_mean(cross_entropy) * 100\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y_hat, 1), tf.argmax(y_true, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "########################################\n",
    "# define the optimizer\n",
    "########################################\n",
    "lr = 0.003\n",
    "optimizer = tf.train.AdamOptimizer(lr)\n",
    "train_step = optimizer.minimize(cross_entropy)\n",
    "\n",
    "########################################\n",
    "# execute the model\n",
    "########################################\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for i in range(n_epochs):\n",
    "        # load batch of images and correct answers\n",
    "        batch_X, batch_y = mnist.train.next_batch(batch_size)\n",
    "        \n",
    "        # learning rate decay\n",
    "        if i % 100 == 0:\n",
    "            a, c = sess.run([accuracy, cross_entropy], feed_dict={X: batch_X, y_true: batch_y, pkeep: 1.0})\n",
    "            print('epoch {}: accurecy = {}, loss = {}'.format(i, a, c))\n",
    "\n",
    "        # train\n",
    "        sess.run(train_step, feed_dict={X: batch_X, y_true: batch_y, pkeep: 0.75})\n",
    "        \n",
    "    a, c = sess.run([accuracy, cross_entropy], feed_dict={X: mnist.test.images, y_true: mnist.test.labels, pkeep: 1.0})\n",
    "    print('test data: accurecy = {}, loss = {}'.format(a, c))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we use Keras to build the above network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/1000\n",
      "60000/60000 [==============================] - 172s 3ms/step - loss: 0.1400 - acc: 0.9567 - val_loss: 0.0420 - val_acc: 0.9861\n",
      "Epoch 2/1000\n",
      "60000/60000 [==============================] - 171s 3ms/step - loss: 0.0412 - acc: 0.9875 - val_loss: 0.0269 - val_acc: 0.9912\n",
      "Epoch 3/1000\n",
      "60000/60000 [==============================] - 171s 3ms/step - loss: 0.0297 - acc: 0.9902 - val_loss: 0.0231 - val_acc: 0.9918\n",
      "Epoch 4/1000\n",
      "60000/60000 [==============================] - 153s 3ms/step - loss: 0.0220 - acc: 0.9927 - val_loss: 0.0236 - val_acc: 0.9929\n",
      "Epoch 5/1000\n",
      "  200/60000 [..............................] - ETA: 4:03 - loss: 0.0080 - acc: 1.0000"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-1cacb4e1e9f7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     43\u001b[0m               metrics=['accuracy'])\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Test loss:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1637\u001b[0m           \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1638\u001b[0m           \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1639\u001b[0;31m           validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1640\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1641\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    213\u001b[0m           \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m           \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2984\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2985\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 2986\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   2987\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2988\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "\n",
    "reset_graph()\n",
    "\n",
    "n_epochs = 10\n",
    "batch_size = 100\n",
    "\n",
    "########################################\n",
    "# load the mnist data\n",
    "########################################\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
    "\n",
    "# Rscaling of the pixel values from 0 to 255 to the range between 0 and 1. It improves the learning speed.\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, 10)\n",
    "y_test = keras.utils.to_categorical(y_test, 10)\n",
    "\n",
    "########################################\n",
    "# buid the model\n",
    "########################################\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=[5, 5], activation='relu', input_shape=[28, 28, 1]))\n",
    "model.add(MaxPooling2D(pool_size=[2, 2], strides=2))\n",
    "model.add(Conv2D(64, kernel_size=[5, 5], activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=[2, 2], strides=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Adam(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=n_epochs, verbose=1, validation_data=(x_test, y_test))\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
